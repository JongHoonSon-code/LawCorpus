{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6092f264-c212-4de5-acf9-e21a63d85ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/anaconda3/envs/kobart_py38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from dataset.ipynb\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from kobart import get_pytorch_kobart_model, get_kobart_tokenizer\n",
    "import import_ipynb\n",
    "from dataset import KoBARTSummaryDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79c9239c-bfbc-4492-91ea-8a7a56659f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='KoBART translation')\n",
    "parser.add_argument('--checkpoint_path',\n",
    "                    type=str,\n",
    "                    help='checkpoint path')\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c642d11-8b70-454a-816a-3888815370ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgsBase():\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(\n",
    "            parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--train_file',\n",
    "                            type=str,\n",
    "                            default='data/train.tsv',\n",
    "                            help='train file')\n",
    "\n",
    "        parser.add_argument('--test_file',\n",
    "                            type=str,\n",
    "                            default='data/test.tsv',\n",
    "                            help='test file')\n",
    "\n",
    "        parser.add_argument('--batch_size',\n",
    "                            type=int,\n",
    "                            default=28,\n",
    "                            help='')\n",
    "\n",
    "        parser.add_argument('--max_len',\n",
    "                            type=int,\n",
    "                            default=512,\n",
    "                            help='max seq len')\n",
    "        return parser\n",
    "\n",
    "class KobartSummaryModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_file,\n",
    "                 test_file, tok,\n",
    "                 max_len=512,\n",
    "                 batch_size=8,\n",
    "                 num_workers=5):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.max_len = max_len\n",
    "        self.train_file_path = train_file\n",
    "        self.test_file_path = test_file\n",
    "        if tok is None:\n",
    "            self.tok = get_kobart_tokenizer()\n",
    "        else:\n",
    "            self.tok = tok\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = argparse.ArgumentParser(\n",
    "            parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--num_workers',\n",
    "                            type=int,\n",
    "                            default=5,\n",
    "                            help='num of worker for dataloader')\n",
    "        return parser\n",
    "\n",
    "    # OPTIONAL, called for every GPU/machine (assigning state is OK)\n",
    "    def setup(self, stage):\n",
    "        # split dataset\n",
    "        self.train = KoBARTSummaryDataset(self.train_file_path,\n",
    "                                 self.tok,\n",
    "                                 self.max_len)\n",
    "        self.test = KoBARTSummaryDataset(self.test_file_path,\n",
    "                                self.tok,\n",
    "                                self.max_len)\n",
    "        print(self.train)\n",
    "    def train_dataloader(self):\n",
    "        train = DataLoader(self.train,\n",
    "                           batch_size=self.batch_size,\n",
    "                           num_workers=self.num_workers, shuffle=True)\n",
    "        return train\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val = DataLoader(self.test,\n",
    "                         batch_size=self.batch_size,\n",
    "                         num_workers=self.num_workers, shuffle=False)\n",
    "        return val\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test = DataLoader(self.test,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, shuffle=False)\n",
    "        return test\n",
    "\n",
    "\n",
    "class Base(pl.LightningModule):\n",
    "    def __init__(self, hparams, **kwargs) -> None:\n",
    "        super(Base, self).__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        # add model specific args\n",
    "        parser = argparse.ArgumentParser(\n",
    "            parents=[parent_parser], add_help=False)\n",
    "\n",
    "        parser.add_argument('--batch-size',\n",
    "                            type=int,\n",
    "                            default=4,\n",
    "                            help='batch size for training (default: 96)')\n",
    "\n",
    "        parser.add_argument('--lr',\n",
    "                            type=float,\n",
    "                            default=3e-5,\n",
    "                            help='The initial learning rate')\n",
    "\n",
    "        parser.add_argument('--warmup_ratio',\n",
    "                            type=float,\n",
    "                            default=0.1,\n",
    "                            help='warmup ratio')\n",
    "\n",
    "        parser.add_argument('--model_path',\n",
    "                            type=str,\n",
    "                            default=None,\n",
    "                            help='kobart model path')\n",
    "        return parser\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Prepare optimizer\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(\n",
    "                nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.lr, correct_bias=False)\n",
    "        # warm up lr\n",
    "        num_workers = (self.hparams.gpus if self.hparams.gpus is not None else 1) * (self.hparams.num_nodes if self.hparams.num_nodes is not None else 1)\n",
    "        data_len = len(self.train_dataloader().dataset)\n",
    "        logging.info(f'number of workers {num_workers}, data length {data_len}')\n",
    "        num_train_steps = int(data_len / (self.hparams.batch_size * num_workers) * self.hparams.max_epochs)\n",
    "        logging.info(f'num_train_steps : {num_train_steps}')\n",
    "        num_warmup_steps = int(num_train_steps * self.hparams.warmup_ratio)\n",
    "        logging.info(f'num_warmup_steps : {num_warmup_steps}')\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n",
    "        lr_scheduler = {'scheduler': scheduler, \n",
    "                        'monitor': 'loss', 'interval': 'step',\n",
    "                        'frequency': 1}\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "\n",
    "class KoBARTConditionalGeneration(Base):\n",
    "    def __init__(self, hparams, **kwargs):\n",
    "        super(KoBARTConditionalGeneration, self).__init__(hparams, **kwargs)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(get_pytorch_kobart_model())\n",
    "        self.model.train()\n",
    "        self.bos_token = '<s>'\n",
    "        self.eos_token = '</s>'\n",
    "        self.pad_token_id = 0\n",
    "        self.tokenizer = get_kobart_tokenizer()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attention_mask = inputs['input_ids'].ne(self.pad_token_id).float()\n",
    "        decoder_attention_mask = inputs['decoder_input_ids'].ne(self.pad_token_id).float()\n",
    "        \n",
    "        return self.model(input_ids=inputs['input_ids'],\n",
    "                          attention_mask=attention_mask,\n",
    "                          decoder_input_ids=inputs['decoder_input_ids'],\n",
    "                          decoder_attention_mask=decoder_attention_mask,\n",
    "                          labels=inputs['labels'], return_dict=True)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outs = self(batch)\n",
    "        loss = outs.loss\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        print(\"validation_step Called.\")\n",
    "        outs = self(batch)\n",
    "        loss = outs['loss']\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return (loss)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        losses = []\n",
    "        for loss in outputs:\n",
    "            losses.append(loss)\n",
    "        self.log('val_loss', torch.stack(losses).mean(), prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c85f5b-f256-4017-a207-98af5717b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Base.add_model_specific_args(parser)\n",
    "parser = ArgsBase.add_model_specific_args(parser)\n",
    "parser = KobartSummaryModule.add_model_specific_args(parser)\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "#args = parser.parse_args()   터미널용\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5f7086e-a3b2-43f4-81f5-6294f4f17207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=1, amp_backend='native', amp_level='O2', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, automatic_optimization=None, batch_size=32, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, checkpoint_path=None, default_root_dir='logs', deterministic=False, distributed_backend=None, enable_pl_optimizer=None, fast_dev_run=False, flush_logs_every_n_steps=100, gpus=1, gradient_clip_val=1.0, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=10, max_len=512, max_steps=None, min_epochs=None, min_steps=None, model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=True, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, sync_batchnorm=False, terminate_on_nan=False, test_file='/etc/jupyterhub/pythonex/Paper_2023/Data/valid.txt', tpu_cores=<function _gpus_arg_default at 0x7feae1c7af70>, track_grad_norm=-1, train_file='/etc/jupyterhub/pythonex/Paper_2023/Data/train.txt', truncated_bptt_steps=None, val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /etc/jupyterhub/pythonex/Paper_2023/KoBART/.cache/kobart_base_cased_ff4bda5738.zip\n",
      "using cached model. /etc/jupyterhub/pythonex/Paper_2023/KoBART/.cache/kobart_base_tokenizer_cased_cf74400bce.zip\n"
     ]
    }
   ],
   "source": [
    "args.train_file = \"/etc/jupyterhub/pythonex/Paper_2023/Data/train.txt\"\n",
    "args.test_file = \"/etc/jupyterhub/pythonex/Paper_2023/Data/valid.txt\"\n",
    "args.gradient_clip_val = 1.0\n",
    "args.max_epochs = 10\n",
    "args.default_root_dir = 'logs'\n",
    "args.gpus = 1\n",
    "args.batch_size = 32\n",
    "args.num_workers = 4\n",
    "#args.lr = 0.0001\n",
    "\n",
    "logging.info(args)\n",
    "\n",
    "model = KoBARTConditionalGeneration(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32f7c6f8-c9b5-4e42-9a85-b81330e25c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. /etc/jupyterhub/pythonex/Paper_2023/KoBART/.cache/kobart_base_tokenizer_cased_cf74400bce.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "INFO:lightning:GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "dm = KobartSummaryModule(args.train_file,\n",
    "                    args.test_file,\n",
    "                    None,\n",
    "                    max_len=args.max_len,\n",
    "                    batch_size=args.batch_size,\n",
    "                    num_workers=args.num_workers)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_loss',\n",
    "                                                   dirpath=args.default_root_dir,\n",
    "                                                   filename='model_chp/{epoch:02d}-{val_loss:.3f}',\n",
    "                                                   verbose=True,\n",
    "                                                   save_last=True,\n",
    "                                                   mode='min',\n",
    "                                                   save_top_k=-1,\n",
    "                                                   prefix='kobart_translation')\n",
    "tb_logger = pl_loggers.TensorBoardLogger(os.path.join(args.default_root_dir, 'tb_logs'))\n",
    "lr_logger = pl.callbacks.LearningRateMonitor()\n",
    "trainer = pl.Trainer.from_argparse_args(args, logger=tb_logger,\n",
    "                                        callbacks=[checkpoint_callback, lr_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ace1b-2186-4b65-a53b-26d8f1af9221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "Skipping line 602686: expected 2 fields, saw 3\n",
      "Skipping line 680723: expected 2 fields, saw 3\n",
      "Skipping line 686094: expected 2 fields, saw 3\n",
      "Skipping line 722392: expected 2 fields, saw 3\n",
      "\n",
      "Skipping line 907831: expected 2 fields, saw 3\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1228059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135966\n",
      "<dataset.KoBARTSummaryDataset object at 0x7feae0e02c70>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:number of workers 1, data length 1228059\n",
      "INFO:root:num_train_steps : 383768\n",
      "INFO:root:num_warmup_steps : 38376\n",
      "\n",
      "  | Name  | Type                         | Params\n",
      "-------------------------------------------------------\n",
      "0 | model | BartForConditionalGeneration | 123 M \n",
      "-------------------------------------------------------\n",
      "123 M     Trainable params\n",
      "0         Non-trainable params\n",
      "123 M     Total params\n",
      "495.440   Total estimated model params size (MB)\n",
      "INFO:lightning:\n",
      "  | Name  | Type                         | Params\n",
      "-------------------------------------------------------\n",
      "0 | model | BartForConditionalGeneration | 123 M \n",
      "-------------------------------------------------------\n",
      "123 M     Trainable params\n",
      "0         Non-trainable params\n",
      "123 M     Total params\n",
      "495.440   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]validation_step Called.\n",
      "Validation sanity check:  50%|█████     | 1/2 [00:01<00:01,  1.15s/it]validation_step Called.\n",
      "Epoch 0:  61%|██████    | 25974/42626 [5:26:23<3:29:15,  1.33it/s, loss=0.718, v_num=0, val_loss=15.20, train_loss=0.699]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d124573-2294-4ecf-831b-e766d1afae5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobart_py38",
   "language": "python",
   "name": "kobart_py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
